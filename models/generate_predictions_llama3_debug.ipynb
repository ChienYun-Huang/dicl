{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_series_names = [\n",
    "                           'brownian_motion', \n",
    "                           # 'geometric_brownian_motion',\n",
    "                           # 'noisy_logistic_map',\n",
    "                           # 'logistic_map',\n",
    "                           # 'lorenz_system',\n",
    "                           # 'uncorrelated_gaussian',\n",
    "                           # 'uncorrelated_uniform'\n",
    "                           ]\n",
    "markov_chain_names = ['markov_chain']\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "### Set up directory\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4,5\"\n",
    "from pathlib import Path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "from llama import get_model_and_tokenizer\n",
    "from ICL import MultiResolutionPDF, recursive_refiner, trim_kv_cache, recursive_refiner_preprompt, recursive_refiner_llama\n",
    "\n",
    "# Check if directory exists, if not create it\n",
    "save_path = Path(parent_dir) / 'processed_series_v2'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "# Define the directory where the generated series are stored\n",
    "generated_series_dir = Path(parent_dir) / 'generated_series'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Markov(full_series, llama_size = '13b'):\n",
    "    '''\n",
    "     This function calculates the multi-resolution probability density function (PDF) for a given series.\n",
    "\n",
    "     Parameters:\n",
    "     full_series (str): The series for which the PDF is to be calculated.\n",
    "     llama_size (str, optional): The size of the llama model. Defaults to '13b'.\n",
    "\n",
    "     Returns:\n",
    "\n",
    "    '''\n",
    "    model, tokenizer = get_model_and_tokenizer(llama_size)\n",
    "    states = sorted(set(full_series))\n",
    "    good_tokens = [tokenizer.convert_tokens_to_ids(state) for state in states]\n",
    "    batch = tokenizer(\n",
    "        [full_series], \n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,        \n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        out = model(batch['input_ids'].cpu())\n",
    "    logit_mat = out['logits']\n",
    "    logit_mat_good = logit_mat[:,:,good_tokens].cpu()\n",
    "\n",
    "    return logit_mat_good\n",
    "\n",
    "def calculate_multiPDF(\n",
    "    full_series, prec, mode = 'neighbor', refine_depth = 1, llama_size = '13b', temperature=1.0\n",
    "):\n",
    "    '''\n",
    "     This function calculates the multi-resolution probability density function (PDF) for a given series.\n",
    "\n",
    "     Parameters:\n",
    "     full_series (str): The series for which the PDF is to be calculated.\n",
    "     prec (int): The precision of the PDF.\n",
    "     mode (str, optional): The mode of calculation. Defaults to 'neighbor'.\n",
    "     refine_depth (int, optional): The depth of refinement for the PDF. Defaults to 1.\n",
    "     llama_size (str, optional): The size of the llama model. Defaults to '13b'.\n",
    "\n",
    "     Returns:\n",
    "     list: A list of PDFs for the series.\n",
    "    '''\n",
    "    # if llama_size != '13b':\n",
    "    #     assert False, \"Llama size must be '13b'\"\n",
    "    good_tokens_str = list(\"0123456789\")\n",
    "    print(f\"good_tokens_str: {good_tokens_str}\")\n",
    "    good_tokens = [tokenizer.convert_tokens_to_ids(token) for token in good_tokens_str]\n",
    "    print(f\"good_tokens: {good_tokens}\")\n",
    "    assert refine_depth < prec, \"Refine depth must be less than precision\"\n",
    "    refine_depth = refine_depth - prec\n",
    "    curr = -prec\n",
    "    batch = tokenizer(\n",
    "        [full_series], \n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True        \n",
    "    )\n",
    "    print(f\"batch['input_ids']: shape | {batch['input_ids'].shape}, sample | {batch['input_ids'][0,:10]}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        out = model(batch['input_ids'].cuda(), use_cache=True)\n",
    "    print(f\"out: {list(out.keys())}\")\n",
    "    logit_mat = out['logits']\n",
    "    print(f\"logit_mat: shape | {logit_mat.shape}, sample | {logit_mat[:10]}\")\n",
    "    kv_cache_main = out['past_key_values']\n",
    "    logit_mat_good = logit_mat[:,:,good_tokens].clone()\n",
    "    print(f\"logit_mat_good: shape | {logit_mat_good.shape}, sample | {logit_mat_good[:10]}\")\n",
    "    probs = torch.nn.functional.softmax(logit_mat_good[:,1:,:], dim=-1)\n",
    "    \n",
    "    PDF_list = []\n",
    "    comma_locations = np.sort(np.where(np.array(list(full_series)) == ',')[0])\n",
    "    \n",
    "    print(f\"len coma locations: {comma_locations.shape} | sample: {comma_locations[:10]}\")\n",
    "    print(f\"probs: {probs.shape}, type: {type(probs)}\")\n",
    "    # start_loop_from = 1 if use_instruct else 0\n",
    "    for i in tqdm(range(len(comma_locations))):\n",
    "        PDF = MultiResolutionPDF()\n",
    "        # slice out the number before ith comma\n",
    "        if i == 0:\n",
    "            start_idx = 0\n",
    "        else:\n",
    "            start_idx = comma_locations[i-1]+1\n",
    "        end_idx = comma_locations[i]\n",
    "        # print(f\"start_idx:end_idx {start_idx}:{end_idx}\")\n",
    "        # if end_idx <= probs.shape[1]:\n",
    "        num_slice = full_series[start_idx:end_idx]\n",
    "        prob_slice = probs[0,start_idx:end_idx].cpu().numpy()\n",
    "        ### Load hierarchical PDF \n",
    "        print(f\"prob_slice: {prob_slice.shape}, type: {type(prob_slice)}, sample: {prob_slice[:10]}\")\n",
    "        print(f\"num_slice: {num_slice}, type: {type(num_slice)}\")\n",
    "        PDF.load_from_num_prob(num_slice, prob_slice)\n",
    "\n",
    "        # raise ValueError('test')\n",
    "        \n",
    "        ### Refine hierarchical PDF\n",
    "        seq = full_series[:end_idx]\n",
    "        # cache and full_series are shifted from beginning, not end\n",
    "        end_idx_neg = end_idx - len(full_series)\n",
    "        ### kv cache contains seq[0:-1]\n",
    "        kv_cache = trim_kv_cache(kv_cache_main, end_idx_neg-1)\n",
    "        recursive_refiner_llama(\n",
    "            PDF, seq, curr=curr, main=True, refine_depth=refine_depth, mode=mode, \n",
    "            kv_cache=kv_cache, model=model, tokenizer=tokenizer, good_tokens=good_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        PDF_list += [PDF]\n",
    "\n",
    "        if i==10:\n",
    "            print(f\"start_idx: {start_idx}\")\n",
    "            print(f\"end_idx: {end_idx}\")\n",
    "            print(f\"num_slice: {num_slice}\")\n",
    "            print(f\"prob_slice: {prob_slice}\")\n",
    "            print(f\"PDF_list: shape | {len(PDF_list)}, sample | {PDF_list[:10]}\")\n",
    "    \n",
    "    # release memory\n",
    "    del logit_mat, kv_cache_main\n",
    "    return PDF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47b34c53c21449ca124360e3de3e1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer('7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 5500]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('999')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the data for continuous series and Markov chains\n",
    "continuous_series_task = {}\n",
    "markov_chain_task = {}\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for file in generated_series_dir.iterdir():\n",
    "    # Check if a series is already processed\n",
    "    # if not (save_path / file.name).exists():\\\n",
    "    # Extract the series name from the file name\n",
    "    series_name = file.stem.rsplit('_', 1)[0]\n",
    "    # If the series is a continuous series, load the data into the continuous_series_data dictionary\n",
    "    if series_name in continuous_series_names:\n",
    "        continuous_series_task[file.name] = pickle.load(file.open('rb'))\n",
    "    # If the series is a Markov chain, load the data into the markov_chain_data dictionary\n",
    "    elif series_name in markov_chain_names:\n",
    "        markov_chain_task[file.name] = pickle.load(file.open('rb'))\n",
    "    # If the series name is not recognized, raise an exception\n",
    "    # else:\n",
    "    #     raise Exception(f\"Unrecognized series name: {series_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['brownian_motion_4.pkl', 'brownian_motion_14.pkl', 'brownian_motion_13.pkl', 'brownian_motion_17.pkl', 'brownian_motion_7.pkl', 'brownian_motion_11.pkl', 'brownian_motion_10.pkl', 'brownian_motion_6.pkl', 'brownian_motion_2.pkl', 'brownian_motion_5.pkl', 'brownian_motion_9.pkl', 'brownian_motion_16.pkl', 'brownian_motion_15.pkl', 'brownian_motion_3.pkl', 'brownian_motion_1.pkl', 'brownian_motion_18.pkl', 'brownian_motion_8.pkl', 'brownian_motion_19.pkl', 'brownian_motion_0.pkl', 'brownian_motion_12.pkl'])\n",
      "dict_keys(['markov_chain_8.pkl', 'markov_chain_7.pkl', 'markov_chain_4.pkl', 'markov_chain_3.pkl', 'markov_chain_10.pkl', 'markov_chain_0.pkl', 'markov_chain_14.pkl', 'markov_chain_5.pkl', 'markov_chain_11.pkl', 'markov_chain_6.pkl', 'markov_chain_12.pkl', 'markov_chain_13.pkl', 'markov_chain_9.pkl', 'markov_chain_17.pkl', 'markov_chain_15.pkl', 'markov_chain_1.pkl', 'markov_chain_16.pkl', 'markov_chain_2.pkl'])\n"
     ]
    }
   ],
   "source": [
    "print(continuous_series_task.keys())\n",
    "print(markov_chain_task.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Multi Digit series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 220, 17, 220, 19, 220, 16]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(' 2 4 1')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  brownian_motion_0.pkl\n",
      "full_series: 214,223,21\n",
      "number_of_tokens_original: 2001\n",
      "comma token: [128000, 11]\n",
      "good_tokens_str: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "good_tokens: [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "batch['input_ids']: shape | torch.Size([1, 2001]), sample | tensor([128000,  11584,     11,  12533,     11,  13460,     11,  15666,     11,\n",
      "         15966])\n",
      "out: ['logits', 'past_key_values']\n",
      "logit_mat: shape | torch.Size([1, 2001, 128256]), sample | tensor([[[  6.8789,   8.8047,  12.9609,  ...,  -4.4453,  -4.4453,  -4.4453],\n",
      "         [  2.9844,   2.9414,   2.4766,  ..., -10.4453, -10.4453, -10.4453],\n",
      "         [ -4.3281,  -8.3203,  -3.3828,  ...,  -9.1172,  -9.1172,  -9.1172],\n",
      "         ...,\n",
      "         [  7.8242,   7.2891,   8.2188,  ...,  -5.4727,  -5.4727,  -5.4727],\n",
      "         [  6.1836,   8.6875,   7.1055,  ...,  -3.5059,  -3.5059,  -3.5059],\n",
      "         [  8.0547,   7.3789,   8.3359,  ...,  -5.5000,  -5.5000,  -5.5000]]],\n",
      "       device='cuda:0')\n",
      "logit_mat_good: shape | torch.Size([1, 2001, 10]), sample | tensor([[[ 6.6016,  9.5625,  8.3125,  ...,  7.5781,  7.2891,  7.1953],\n",
      "         [ 8.7812,  8.6328,  8.2969,  ...,  8.1875,  8.3047,  8.0859],\n",
      "         [ 6.1484,  6.4570,  6.7500,  ...,  6.3789,  6.3789,  6.3672],\n",
      "         ...,\n",
      "         [11.9062, 12.3594, 11.0078,  ..., 11.2891, 12.8750, 11.3828],\n",
      "         [10.7734, 10.7344,  9.7500,  ..., 10.4375, 10.5234, 10.5781],\n",
      "         [11.8359, 12.1562, 10.7031,  ..., 10.6172, 12.4609, 11.3125]]],\n",
      "       device='cuda:0')\n",
      "len coma locations: (1000,) | sample: [ 3  7 11 15 19 23 27 31 35 39]\n",
      "probs: torch.Size([1, 2000, 10]), type: <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_slice: (3, 10), type: <class 'numpy.ndarray'>, sample: [[0.15000482 0.12931223 0.09241529 0.08414508 0.1022944  0.11959427\n",
      "  0.071413   0.08284052 0.09314012 0.07484019]\n",
      " [0.06728137 0.0916042  0.12278635 0.07865989 0.08505161 0.21803695\n",
      "  0.08340657 0.08472003 0.08472003 0.08373301]\n",
      " [0.07188363 0.20239165 0.07956792 0.08145482 0.0735883  0.08773029\n",
      "  0.06994469 0.12420376 0.08503111 0.12420376]]\n",
      "num_slice: 214, type: <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_of_tokens_original: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_tokens_original\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomma token: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m PDF_list \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_multiPDF\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_series\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefine_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrefine_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllama_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m series_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPDF_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PDF_list\n\u001b[1;32m     24\u001b[0m save_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseries_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_llama3_temperature.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m, in \u001b[0;36mcalculate_multiPDF\u001b[0;34m(full_series, prec, mode, refine_depth, llama_size, temperature)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_slice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_slice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(num_slice)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m PDF\u001b[38;5;241m.\u001b[39mload_from_num_prob(num_slice, prob_slice)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m### Refine hierarchical PDF\u001b[39;00m\n\u001b[1;32m     96\u001b[0m seq \u001b[38;5;241m=\u001b[39m full_series[:end_idx]\n",
      "\u001b[0;31mValueError\u001b[0m: test"
     ]
    }
   ],
   "source": [
    "number_of_tokens_original = None\n",
    "for series_name, series_dict in sorted(continuous_series_task.items()):\n",
    "    print(\"Processing \", series_name)\n",
    "    if 'brownian_motion' in series_name:\n",
    "        full_series = series_dict['full_series']\n",
    "        # add spaces in the series\n",
    "        # print(type(full_series))\n",
    "        # str_fill = [' ' + str(char) if str(char) != ',' else str(char) for char in full_series]\n",
    "        # full_series = \"\"\n",
    "        # for char in str_fill:\n",
    "        #     full_series += char\n",
    "        print(f\"full_series: {full_series[:10]}\")\n",
    "        prec = series_dict['prec']\n",
    "        refine_depth = series_dict['refine_depth']\n",
    "        llama_size = series_dict['llama_size']\n",
    "        mode = series_dict['mode']\n",
    "        number_of_tokens_original = len(tokenizer(full_series)['input_ids'])\n",
    "        print(f\"number_of_tokens_original: {number_of_tokens_original}\")\n",
    "        print(f\"comma token: {tokenizer(',')['input_ids']}\")\n",
    "        PDF_list = calculate_multiPDF(\n",
    "            full_series, prec, mode = mode, refine_depth = refine_depth, llama_size = llama_size, temperature=2.5\n",
    "        )\n",
    "        series_dict['PDF_list'] = PDF_list\n",
    "        save_name = os.path.join(save_path, f\"{series_name.split('.')[0]}_llama3_temperature.pkl\")\n",
    "        # save_name = os.path.join(save_path, series_name)\n",
    "        with open(save_name, 'wb') as f:\n",
    "            pickle.dump(series_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Markov Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  markov_chain_0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "for series_name, series_dict in sorted(markov_chain_task.items()):\n",
    "    print(\"Processing \", series_name)\n",
    "    full_series = series_dict['full_series']\n",
    "    llama_size = series_dict['llama_size']\n",
    "    logit_mat_good = calculate_Markov(full_series, llama_size = llama_size)    \n",
    "    series_dict['logit_mat_good'] = logit_mat_good\n",
    "    save_name = os.path.join(save_path, series_name)\n",
    "    with open(save_name, 'wb') as f:\n",
    "        pickle.dump(series_dict, f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LLMICL]",
   "language": "python",
   "name": "conda-env-LLMICL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
