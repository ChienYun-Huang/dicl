import numpy as np


def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        policy: np.array
            matrix mapping states to action (Ns)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        value_function: np.array
            The value function of the given policy
    """
    Ns, Na = R.shape
    value_function = np.zeros(Ns)
    # ====================================================
	# YOUR IMPLEMENTATION HERE 
    #
    Rs = [R[i,policy[i]] for i in range(Ns)]
    gammaP = [[gamma*P[i,policy[i],j] for j in range(Ns)] for i in range(Ns)]
    value_function = np.linalg.solve(np.identity(int(Ns)) - gammaP,Rs)
    # ====================================================
    return value_function

def policy_iteration(P, R, gamma=0.9, tol=1e-3):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        policy: np.array
            the final policy
        V: np.array
            the value function associated to the final policy
    """
    Ns, Na = R.shape
    V = np.zeros(Ns)
    policy = np.zeros(Ns, dtype="int")
    # ====================================================
	# YOUR IMPLEMENTATION HERE 
    #
    cond = True
    iteration = 0
    while cond:
        Vk = policy_evaluation(P, R, policy, gamma, 1e-2)
        for s in range(Ns):
            act = np.zeros(Na)
            for a in range(Na):
                inter = 0
                for sprime in range (Ns):
                    inter += gamma*Vk[sprime]*P[s,a,sprime]
                act[a] = inter
            vector = R[s,:] + act
            policy[s] = np.argmax(vector) 
        if np.linalg.norm(Vk - V) < tol:    
            cond = False
        V = Vk
        iteration += 1
    V = policy_evaluation(P, R, policy, gamma, 1e-2)
    # ====================================================
    return policy, V, iteration

def value_iteration(P, R, gamma=0.9, tol=1e-3):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        Q: final Q-function (at iteration n)
        greedy_policy: greedy policy wrt Qn
        Qfs: all Q-functions generated by the algorithm (for visualization)
    """
    Ns, Na = R.shape
    Q = np.zeros((Ns, Na))
    Qfs = [Q]
    # ====================================================
    # YOUR IMPLEMENTATION HERE 
    #
    cond = True
    iteration = 0
    while cond:
        Qnew = np.zeros((Ns, Na))
        Qmax = np.max(Q,axis=1)
        Qnew = np.tensordot(P,Qmax,axes=1) 
        Qnew = R + gamma * Qnew
        Qfs.append(Qnew)
        if np.linalg.norm(Qnew - Q, ord=np.inf) <= tol:
            cond = False
        Q = Qnew
        iteration += 1
    greedy_policy = np.argmax(Q, axis=1)
    # ====================================================
    return Q, greedy_policy, Qfs, iteration

def compute_optimal_policy(T, R, gamma, algo='VI', tol=1e-2):
    if algo == 'VI':
        _, policy, _, iteration = value_iteration(P=T, R=R, gamma=gamma, tol=tol)
    elif algo == 'PI':
        policy, _, iteration = policy_iteration(P=T, R=R, gamma=gamma, tol=tol)
    else:
        raise ValueError('algo not supported!')
    return policy, iteration